# Determination of Toxic Comments and Analysis for the Minimization of Unintentional Model Bias
The primary goal of this project is to design a supervised and deep learning model that can perform binary classification in order to determine toxic and non-toxic comments from a social network.

Online conversations can be toxic and subjected to threats, abuse, or harassment. Online conversation toxicity can be defined as rude, disrespectful, make somebody leave a discussion, stop expressing oneself, and even give up on looking for dfferent opinions. To help improving online onversation and analyze the negative online ehaviors, the goal of this project is to build a supervised machine learning model that can perform binary classification, determine toxic and non-toxic comments, and analysis for the minimization of the unintended model bias concerning identity features such as race, gender, sex, religion, and disability, etc.

The dataset is collected by the Conversation AI [1] team, a research initiative founded by Jigsaw [2] and Google organized a Kaggle competition in 2019: Jigsaw Unintended Bias in Toxicity Classification [3]. The dataset was collected from Civil Comments platform from 2015 to 2017.
